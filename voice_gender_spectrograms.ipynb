{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZjbYLH8iESU"
      },
      "source": [
        "# K.c Kula Computational Methods in the Sciences Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqMkxvudiPBj"
      },
      "source": [
        "The following is a final project for my Computational Methods in the Sciences course. Project guiding question: Can neural networks predict a speaker's gender(as defined in the dataset) using Mel Spectrograms and neural networks? Does the language impact classification accuracy?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-GSZ-XZmdIU"
      },
      "source": [
        "## Brief Summary\n",
        "This project found that using a neural network on 2 second clips produces slightly higher than average accuracy on the test data. The German data performed slightly better, although future improvements could include: longer clips, different method(CNN), including \"other\" voices, and/or multiple speakers in one clip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUfHCD6g6i2r"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ZMheWtIEmx"
      },
      "source": [
        "## Overall\n",
        "Data are from  Mozilla Common Voice Corpus 7.0 English and German Files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNCcL40BlhxF"
      },
      "source": [
        "This project relied heavily on the following 2 sources. This is also denoted when used.\n",
        "*  https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505\n",
        "*   Chat GPT\n",
        "\n",
        "I also talked to members of LingoPal, a startup specializing in real-time audio and video translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJhA_096lmd"
      },
      "source": [
        "## Imports:\n",
        "\n",
        "\n",
        "*   https://favtutor.com/articles/pandas-read-tsv/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2ly7NxvfSVq"
      },
      "source": [
        "## Preprocessing Labels\n",
        "*   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
        "*   https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
        "*   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "*   Chat GPT\n",
        "*   https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiWCEm9fvlFP"
      },
      "source": [
        "##Pre-Preprocessing Files\n",
        "\n",
        "\n",
        "*   https://pandas.pydata.org/docs/user_guide/merging.html\n",
        "*   Chat GPT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnx9_BCTMYgY"
      },
      "source": [
        "## Preprocessing Files\n",
        "\n",
        "\n",
        "*   Chat GPT\n",
        "*   https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505\n",
        "*   https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html\n",
        "*  https://www.geeksforgeeks.org/working-with-wav-files-in-python-using-pydub/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSrZuZm6cmGx"
      },
      "source": [
        "## German and English File Processing\n",
        "\n",
        "*   Chat GPT\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
        "* https://stackoverflow.com/questions/75979632/pytorchs-nn-bcewithlogitsloss-behaves-totaly-differently-than-nn-bceloss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtVCyJ8h6SJ-"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwjvuwoseVg",
        "outputId": "b5ad1101-cd2a-4436-9926-174b2bc40fa9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "%pip install pydub\n",
        "from pydub import AudioSegment\n",
        "from scipy import signal\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "# Google Drive imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#Reads in training and testing files for both languages\n",
        "#Imports tsv file from Drive  # https://favtutor.com/articles/pandas-read-tsv/\n",
        "df_de_train = pd.read_csv('de/train.tsv', sep='\\t')\n",
        "df_de_test = pd.read_csv('de/test.tsv', sep='\\t')\n",
        "df_en_train = pd.read_csv('en/train.tsv', sep='\\t')\n",
        "df_en_test = pd.read_csv('en/test.tsv', sep='\\t')\n",
        "\n",
        "# Path to dataset\n",
        "path_to_datasets = 'path_to_dataset'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LamA7A80Y6D8"
      },
      "source": [
        "replace_mp3_on_set is a function that changes end of file name in dataframe to .wav from .mp3. We need to do this, as we expect our audio files will be .wav. This function takes in a filename and converts the label to end in .wav(ex. file 123456.mp3 and turns it into 123456.wav). This does not change the actual file type, as that will be in the preprocessing audio section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edl0uwjf80f9"
      },
      "outputs": [],
      "source": [
        "#Replaces the file name to end in .wav in the table so we can accuratly use the file name when it is time to compare\n",
        "#Used GPT for the idea of how to replace in a dataframe.\n",
        "def replace_mp3_on_set(filename):\n",
        "  filename_trim = filename[:-3] #Takes out the .mp3\n",
        "  filename_csv_end = filename_trim + \"wav\" #Replaces it with .wav\n",
        "  return filename_csv_end #Returns the new file name in our table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGFu5yAcp5m"
      },
      "source": [
        "Below is the code that applied the above function to the dataframes containing the labels. The dataframe is then trimmed so only columns path and gender remain (path = filename, gender = voice identified as male, female, na, or other). Next, the data are included only if the voices are categorized as male or female(excludes na and other). Finally, the head of the dataframes are printed to confirm the changes appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPzyqm-r82IH",
        "outputId": "24bad06e-b5f6-4580-94c1-5a68c3529a49"
      },
      "outputs": [],
      "source": [
        "#German train replacement\n",
        "df_de_train['path'] = df_de_train['path'].apply(replace_mp3_on_set) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
        "trimmed_df_de_train = df_de_train[['path', 'gender']] #https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
        "trimmed_df_de_train_gender = trimmed_df_de_train[trimmed_df_de_train['gender'].isin(['male', 'female'])] # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "\n",
        "#German test replacement\n",
        "df_de_test['path'] = df_de_test['path'].apply(replace_mp3_on_set) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
        "trimmed_df_de_test = df_de_test[['path', 'gender']] #https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
        "trimmed_df_de_test_gender = trimmed_df_de_test[trimmed_df_de_test['gender'].isin(['male', 'female'])] # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "\n",
        "#English train replacement\n",
        "df_en_train['path'] = df_en_train['path'].apply(replace_mp3_on_set) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
        "trimmed_df_en_train = df_en_train[['path', 'gender']] #https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
        "trimmed_df_en_train_gender = trimmed_df_en_train[trimmed_df_en_train['gender'].isin(['male', 'female'])] # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "\n",
        "#English test replacement\n",
        "df_en_test['path'] = df_en_test['path'].apply(replace_mp3_on_set) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
        "trimmed_df_en_test = df_en_test[['path', 'gender']] #https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
        "trimmed_df_en_test_gender = trimmed_df_en_test[trimmed_df_en_test['gender'].isin(['male', 'female'])] # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html\n",
        "\n",
        "#Prints the first few rows of the DataFrame to check changes\n",
        "print(trimmed_df_de_train_gender.head())\n",
        "print(trimmed_df_de_test_gender.head())\n",
        "print(trimmed_df_en_train_gender.head())\n",
        "print(trimmed_df_en_test_gender.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CSEnlC2hFaf"
      },
      "source": [
        "Unfortunatly, the data are extremely imbalanced. To combat this, I undersampled the males. I also undersampled all of the training and test sets to limit the number of files I had to process. To do this, I initially found the counts of the German and English training and test sets by gender and then chose to have 200 samples per gender in both language's training sets and 40 samples per gender in the test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZxDiq3jGbGC"
      },
      "outputs": [],
      "source": [
        "#Balanced datasets so male samples dont overpower female samples\n",
        "#Used GPT to randomly sample from the train and test\n",
        "\n",
        "def balance_male_female_train(df): #This handles the train data\n",
        "    # Filters rows for males and females\n",
        "    males = df[df['gender'] == 'male']\n",
        "    females = df[df['gender'] == 'female']\n",
        "\n",
        "    # Get the smaller count. Likely female, but uses min to confirm this.\n",
        "    min_count = min(len(males), len(females))\n",
        "\n",
        "    # Balance both genders by sampling. To limit the files to process, if the minimum for the genders is greater than 200, only 200 samples are used to limit files processed.\n",
        "    if min_count > 200:\n",
        "      min_count = 200\n",
        "    balanced_males = males.sample(n=min_count, random_state=12) #Randomly selects n number of files to be used where n = min_count(200)\n",
        "    balanced_females = females.sample(n=min_count, random_state=12)\n",
        "\n",
        "    # Combines balanced datasets to return the full training set.\n",
        "    balanced_df = pd.concat([balanced_males, balanced_females])\n",
        "    return balanced_df\n",
        "\n",
        "def balance_male_female_test(df): #Same as above only with test set, using 40 samples\n",
        "    # Filter rows for males and females\n",
        "    males = df[df['gender'] == 'male']\n",
        "    females = df[df['gender'] == 'female']\n",
        "\n",
        "    # Get the smaller count\n",
        "    min_count = min(len(males), len(females))\n",
        "\n",
        "\n",
        "    if min_count > 40:\n",
        "      min_count = 40\n",
        "    balanced_males = males.sample(n=min_count, random_state=42)\n",
        "    balanced_females = females.sample(n=min_count, random_state=42)\n",
        "\n",
        "    # Combines balanced datasets to return the full testing set.\n",
        "    balanced_df = pd.concat([balanced_males, balanced_females])\n",
        "    return balanced_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76j9TxQtKdW"
      },
      "source": [
        "Below applied the functions above and prints the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM0IzEFutOTp",
        "outputId": "ad6792da-4d3d-40f3-ff9c-5f17520a86f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply the function to each dataset\n",
        "balanced_de_train = balance_male_female_train(trimmed_df_de_train_gender)\n",
        "balanced_de_test = balance_male_female_test(trimmed_df_de_test_gender)\n",
        "balanced_en_train = balance_male_female_train(trimmed_df_en_train_gender)\n",
        "balanced_en_test = balance_male_female_test(trimmed_df_en_test_gender)\n",
        "\n",
        "\n",
        "# Print to confirm that the gender counts accross languages are consistant. #Used gpt for help with creating new line and counting items easily\n",
        "print(\"Balanced German Train Gender Counts:\")\n",
        "print(balanced_de_train['gender'].value_counts()) #https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n",
        "\n",
        "print(\"\\nBalanced German Test Gender Counts:\") #\\n creates new line.\n",
        "print(balanced_de_test['gender'].value_counts())\n",
        "\n",
        "print(\"\\nBalanced English Train Gender Counts:\")\n",
        "print(balanced_en_train['gender'].value_counts())\n",
        "\n",
        "print(\"\\nBalanced English Test Gender Counts:\")\n",
        "print(balanced_en_test['gender'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H49SimVtRcZ"
      },
      "source": [
        "We then need to put our labels into a form we can use. We are thus making a column vector of zeros and ones, where zero indictes male and 1 indicates female. We can do this since it is binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7n5anWQTyvF"
      },
      "outputs": [],
      "source": [
        "german_labels_train = torch.zeros([len(balanced_de_train),1]) #Creates empty array of torch zeros to be filled later. It must be the size of the german labels training by 1(since 2 classifications(male, female) can do binary)\n",
        "german_labels_test = torch.zeros([len(balanced_de_test),1]) #Does the same as above with test\n",
        "\n",
        "#For training data #Used GPT to help transform it from our in class code from lab 8 exersize 1.2 to something that handles binary data\n",
        "for i in range(len(balanced_de_train)): #Iterates across balanced_de_train length\n",
        "    if balanced_de_train['gender'].iloc[i] == 'female': #Uses iloc since the male/female are not numbers and thus cannot just use ['gender][i] == 'male'. Checks if the gender is female\n",
        "        german_labels_train[i] = 1  # The column value to 1 if it is a female. Otherwise it will remain zero, since we filled the array with zeros.\n",
        "\n",
        "#Does the same for testing data\n",
        "for i in range(len(balanced_de_test)):\n",
        "    if balanced_de_test['gender'].iloc[i] == 'female':\n",
        "        german_labels_test[i] = 1\n",
        "\n",
        "\n",
        "\n",
        "# Does the same with English\n",
        "english_labels_train = torch.zeros([len(balanced_en_train),1])\n",
        "english_labels_test = torch.zeros([len(balanced_en_test),1])\n",
        "\n",
        "#For training data\n",
        "for i in range(len(balanced_en_train)):\n",
        "    if balanced_en_train['gender'].iloc[i] == 'female':\n",
        "        english_labels_train[i] = 1\n",
        "\n",
        "\n",
        "#For testing data\n",
        "for i in range(len(balanced_en_test)):\n",
        "    if balanced_en_test['gender'].iloc[i] == 'female':\n",
        "        english_labels_test[i] = 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PzJ7A3a1WK0b",
        "outputId": "855e8a35-374e-43c1-dfb9-7af94050205c"
      },
      "outputs": [],
      "source": [
        "#Test that everything looks ok for German -- assume same for English. These are the same checks used in lab 8, 1.3\n",
        "print(torch.sum(german_labels_train)) #prints sum of all values of matrix(should be half length of the training labels, since theres exactly half female, half male\n",
        "print(len(balanced_de_train))\n",
        "\n",
        "#Testing confirmation. Should be same ratio(1/2)\n",
        "print(torch.sum(german_labels_test))\n",
        "print(len(balanced_de_test))\n",
        "\n",
        "# Confirms that there is only 1 or 0. for each sample. Also confirms that every row has a 1.\n",
        "torch.max(torch.sum(german_labels_train, axis = 1)) #Max should be 1\n",
        "torch.min(torch.sum(german_labels_train, axis = 1)) #Min should be 0\n",
        "torch.mean(torch.sum(german_labels_train, axis = 1)) #Mean should be exactly .5 since half male(0), half female(1)\n",
        "\n",
        "torch.max(torch.sum(german_labels_test, axis = 1)) #Max should be 1\n",
        "torch.min(torch.sum(german_labels_test, axis = 1)) #Min should be 0\n",
        "torch.mean(torch.sum(german_labels_test, axis = 1)) #Mean should be exactly .5 since half male(0), half female(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YTfUf_Lmc8-"
      },
      "source": [
        "#Pre-Preproccessing Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n8Vab8zmhuz"
      },
      "source": [
        "While we know the file names we want to import from above, we need to put them in a format where we can check if the file should be imported. To do this, we will put all of our filenames to be imported into a dataframe and then a list. We also know that the audio files are in mp3 form, so we need to edit the extension of the files we want. This selction uses the same sources as the replace_mp3_on_set above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c2zh9fSmhFr"
      },
      "outputs": [],
      "source": [
        "files_to_import = pd.concat([balanced_de_train, balanced_de_test, balanced_en_train, balanced_en_test]) # https://pandas.pydata.org/docs/user_guide/merging.html\n",
        "\n",
        "def replace_wav_on_set(filename): #Changes file name back to .mp3\n",
        "  filename_trim = filename[:-3] #Takes out the .wav\n",
        "  filename_csv_end = filename_trim + \"mp3\" #Replaces it with .mp3\n",
        "  return filename_csv_end #Returns the new file name in our table\n",
        "files_to_import['path'] = files_to_import['path'].apply(replace_wav_on_set)\n",
        "files_to_import_list = files_to_import['path'].tolist()  # Converts import path column to list to make it easier to preprocess files #from GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3DjCGYHG5Bj"
      },
      "source": [
        "#Preprocessing Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERkHwVkueJs"
      },
      "source": [
        "To save space on my drive, the files were uploaded as zips. We need to do the following to get the files into this notebook.\n",
        "\n",
        "*  Unzip the files into a new folder called clips.\n",
        "*  Only unzip files that we want to unzip(otherwise we will unzip 30,000 files)\n",
        "*  Ensures that we are in the correct folder(/clips/clips), since we created a new folder clips inside clips.\n",
        "*   Checks that files to convert to .wav are in the list of files to import.\n",
        "*   Convert .mp3 files to .wav. This is NOT the same as above, as we are actually changing the file type here, not just the name.\n",
        "*   Pad or trim the files to 2 seconds.\n",
        "* Deletes any mp3 files, so the drive only contains the wav files to save space.\n",
        "*   Ensure that the files were correctly imported.\n",
        "\n",
        "\n",
        "This section used Chat GPT to import and pad the files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxwjJOoPcgf9",
        "outputId": "d98f793a-29a5-4beb-8bf2-f8895f506ceb"
      },
      "outputs": [],
      "source": [
        "# Folders to be accessed in drive (German(de) and English (en))\n",
        "folders = ['de', 'en']\n",
        "pad_ms = 2000  # Desired length of audio file(2 seconds)\n",
        "\n",
        "for folder in folders: #Iterates through both folders\n",
        "    #Unzips files\n",
        "    unzip_destination = os.path.join(path_to_datasets, folder, 'clips') #Defines where in drive\n",
        "    zip_file_path = os.path.join(unzip_destination, 'clips.zip') # Says which file to unzip\n",
        "\n",
        "   #Below unzips files that have the same name as a file in files_to_import list\n",
        "    if os.path.exists(zip_file_path): #Checks to make sure the import path exists\n",
        "        print(f\"Unzipping {zip_file_path} to {unzip_destination}\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            files_in_zip = zip_ref.namelist()\n",
        "            files_to_extract = [f for f in files_in_zip if os.path.basename(f) in files_to_import_list]  # Unzip only files listed in 'files_to_import'\n",
        "            if not files_to_extract:\n",
        "                print(f\"No matching files to extract from {zip_file_path}\")\n",
        "            else:\n",
        "                for file in files_to_extract:\n",
        "                    zip_ref.extract(file, unzip_destination) #Unzips files in list\n",
        "                print(f\"Extracted files: {files_to_extract}\")\n",
        "    else:\n",
        "        print(f\"Zip file not found at {zip_file_path}\") #Prints not found if there was no file found in appropriate location\n",
        "\n",
        "    # Converts mp3 files to wav and make them all the same length\n",
        "    folder_path = os.path.join(path_to_datasets, folder, 'clips/clips') #Defines which folder (clips/clips since we made a new folder)\n",
        "    print(folder_path)\n",
        "    mp3_files = glob.glob(os.path.join(folder_path, '*.mp3')) #Says which mp3 files\n",
        "\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    print(f\"MP3 files found: {mp3_files}\")\n",
        "\n",
        "    for mp3_file in mp3_files: #Iterates accross mp3 files\n",
        "\n",
        "        if os.path.basename(mp3_file) in files_to_import_list:# Checks again if the file is listed in the 'files_to_import' dataframe\n",
        "            wav_file = mp3_file.replace('.mp3', '.wav') #Defines file name as wav\n",
        "            print(f\"Converting: {mp3_file} -> {wav_file}\") #Prints that the conversion is happeneing for the file\n",
        "\n",
        "            try:\n",
        "\n",
        "                audio = AudioSegment.from_mp3(mp3_file) # Loads mp3 file and export as wav\n",
        "\n",
        "                # Pad or trim to make the audio exactly 2 seconds\n",
        "                if len(audio) < pad_ms: #Checks if the length of the audio is less than 2 seconds\n",
        "                    silence = AudioSegment.silent(duration=pad_ms - len(audio)) #If the length is less than 2, adds extra seconds of silence\n",
        "                    padded = audio + silence #The new audio is the audio and the silence together\n",
        "                else:\n",
        "                    padded = audio[:pad_ms] #Otherwise, the padded audio is the first 2 seconds\n",
        "\n",
        "                # Export the padded audio as a .wav file\n",
        "                padded.export(wav_file, format='wav') #Exports the file as a wav\n",
        "\n",
        "\n",
        "                os.remove(mp3_file)  # Keeps the wav files instead of both wav and mp3(to save storage)\n",
        "\n",
        "            except Exception as e: #If it has an error when converting, the program throws an exception\n",
        "                print(f\"Error converting {mp3_file}: {e}\")\n",
        "\n",
        "    print(f\"Conversion and padding complete for folder: {folder}\") #Prints when one folder is complete\n",
        "\n",
        "#Checks the output files for make sure everything is where it should be\n",
        "for folder in folders: #Checks both folders\n",
        "    folder_path = os.path.join(path_to_datasets, folder, 'clips/clips') #Checks within the specifc folder in drive\n",
        "    wav_files = glob.glob(os.path.join(folder_path, '*.wav')) #Checks for specifc file\n",
        "    print(f\"WAV files in folder '{folder}/clips': {wav_files}\") #Lists the files that are found in the folder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA82Ij23BPGO"
      },
      "source": [
        "## Example Plots of Mel Spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8VROL37zU6I"
      },
      "source": [
        "Below plots spectrograms in both regular spectrogram and Mel spectrogram form. We use a mel spectrogram as it is better to train a neutral net on them. Mel spectrograms use the Mel scale, which takes how we perceieve human frequncies into account. Mel spectrograms use the Mel scale instead of frequency, and the decibel scale instead of amplitude. To view the spectrograms, we are only going to look at German, but assume English looks similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wUGb5_uEPR1c",
        "outputId": "aa110efc-f8f2-4f46-fc69-1a1b944b8104"
      },
      "outputs": [],
      "source": [
        "folder = 'de' #Looks at German folder\n",
        "folder_path = os.path.join(path_to_datasets, folder, 'clips/clips')  #Updates path (into clips/clips, since we created new folder clips)\n",
        "wav_files = sorted(glob.glob(os.path.join(folder_path, '*.wav')))  #Creates a sorted list of the wav_files\n",
        "\n",
        "for i in range(5): #Prints 5 images of each\n",
        "  if wav_files: #Checks to make sure its not an empty list\n",
        "      first_wav_file = wav_files[i]  # Get the i-th .wav file(here we plot 5)\n",
        "\n",
        "      #Loads the files using pydub\n",
        "      try:\n",
        "          audio = AudioSegment.from_wav(first_wav_file) #Loads in audio files, creates AudioSegment object with the file\n",
        "          print(f\"Audio duration: {len(audio)} ms\")\n",
        "      except Exception as e: #Prints exception if error converting it to AudioSegment object\n",
        "          print(f\"Error loading audio: {e}\")\n",
        "          audio = None\n",
        "\n",
        "      if audio:\n",
        "          #If the audio file exists, we will create an array of the information and then plot it on the spectrogram.\n",
        "          samples = np.array(audio.get_array_of_samples()) #Creates an array of audio information using built in pydub function\n",
        "          print(f\"Number of samples: {len(samples)}\") # Prints number of samples for the audio file.\n",
        "\n",
        "          sampFreq = audio.frame_rate  # Uses built in function audio.frame rate to find frequency\n",
        "          print(f\"Sampling frequency: {sampFreq} Hz\")  # Prints sampling frequency that we found.\n",
        "\n",
        "\n",
        "          f, t, Sxx = signal.spectrogram(samples, sampFreq) #Calculates the spectrogram\n",
        "\n",
        "          # Plot the spectrogram using code from class\n",
        "          plt.pcolormesh(t, f, Sxx, shading='gouraud') #Plots the spectrogram\n",
        "          plt.ylabel('Frequency (Hz)') #Plots y label\n",
        "          plt.xlabel('Time (sec)') #Plots x label\n",
        "          plt.ylim([0, 1000]) #Limits y axis\n",
        "          plt.xlim([0,2]) #Limits x axis to 2 seconds\n",
        "          plt.colorbar() #Creates a color bar of amplitude\n",
        "          plt.title(\"Spectrogram of Audio File\" + str(first_wav_file)) #Creates title\n",
        "          plt.show()\n",
        "\n",
        "          #GPT to help normalize samples for input into spectrogram\n",
        "          samples = samples.astype(float) # pydub samples are in integer format, this converts them to float\n",
        "          samples = samples / (2**15)  #Normalizes the array to float values between -1 and 1 for librosa\n",
        "\n",
        "          #Plot mel spectogram using GPT and https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505\n",
        "          # and https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html\n",
        "\n",
        "          # Generate the Mel spectrogram\n",
        "          n_fft = 2048 #Uses 2048 samples for each fft(window for each fft, but overall is stft)\n",
        "          S = librosa.feature.melspectrogram(y=samples, sr=sampFreq, n_fft = n_fft) #https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505\n",
        "          fig, ax = plt.subplots()\n",
        "          S_dB = librosa.power_to_db(S, ref=np.max)# Convert to decibals\n",
        "          img = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sampFreq, fmax=8000, ax=ax) #Plots spectrogram. 8000 hz is typcal max for human voice\n",
        "          plt.colorbar(img, ax=ax, format='%+2.0f dB')# Add a color bar to the plot\n",
        "          plt.title('Mel-frequency spectrogram for  of Audio File' + str(first_wav_file))\n",
        "          plt.show()\n",
        "  else:\n",
        "      print(\"No .wav files found in the specified folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PmqmsDkBUir"
      },
      "source": [
        "## Conversion of Audio file to Mel Spectrogram to NP array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI0KmmGRovel"
      },
      "source": [
        "We first define two functions: one to convert the audio segments into mel spectrograms by one file(convert_to_mel_spectrogram), and the other that iterates over the dataframe and dynamically pads the spectrograms by calling the first function(process_audio_files_from_df). By calling the second function, we call the first function and output a numpy array. Much of this section was taken from above or help with GPT when dealing with padding issues, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnQE5aI_6JMu"
      },
      "outputs": [],
      "source": [
        "#This function does very similar to above, but also repads to exactly 2 seconds again.\n",
        "def convert_to_mel_spectrogram(audio_file_path, n_mels=128, target_length=2000): #Function to convert one file to mel spedtrograms. n_mels = 128 controls frequency resolution. Will be a final dim of output\n",
        "    try: #help from GPT to catch errors without completely stopping code\n",
        "        audio = AudioSegment.from_file(audio_file_path)\n",
        "\n",
        "        # Confirms length is exactly 2 seconds(errors if not). Pads again if not.\n",
        "        current_length = len(audio)\n",
        "        if current_length < target_length:\n",
        "            audio += AudioSegment.silent(duration=(target_length - current_length))\n",
        "        elif current_length > target_length:\n",
        "            audio = audio[:target_length]\n",
        "\n",
        "        samples = np.array(audio.get_array_of_samples())#Creates an array for the samples\n",
        "\n",
        "        samples = samples.astype(np.float32) / (2**15)# Normalize the sample array to float values between -1 and 1 for librosa\n",
        "\n",
        "        sampFreq = audio.frame_rate#Finds sampling frequency using frame_rate, which is a built in pydub function https://www.geeksforgeeks.org/working-with-wav-files-in-python-using-pydub/\n",
        "\n",
        "        #Calls mel spectrogram\n",
        "        S = librosa.feature.melspectrogram(y=samples, sr=sampFreq, n_mels=n_mels) #Input into librosa is audio samples for 1 file, the sampling frequnecy calculated using .frame_rate\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)# Convert to decibals(This takes the place of amplitute on a regular spectrogram)\n",
        "\n",
        "        #Converts time to frames and then pads if needed.\n",
        "        expected_frames = librosa.time_to_frames(target_length / 1000, sr=sampFreq) #Converts time value to frames\n",
        "        if S_dB.shape[1] < expected_frames:# Pads for correct number of frames: Uses GPT\n",
        "            S_dB = np.pad(S_dB, ((0, 0), (0, expected_frames - S_dB.shape[1])), 'constant', constant_values=-80) #Value of -80 is typical of mel spectrogram(silence)\n",
        "        return S_dB\n",
        "\n",
        "    except Exception as e: #prints exception if theres an error\n",
        "        print(f\"Error processing {audio_file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_audio_files_from_df(df, folder_path, n_mels=128, target_length=2000):#Processes all audio files. Much of the function was generated through GPT.\n",
        "    #Calculates max expected frames for uniform spectrogram size(so all spectrograms are of same size)\n",
        "    max_frames = None #Sets max frames to none(start with no max)\n",
        "    spectrograms = [] #Empty list to store spectrograms\n",
        "\n",
        "    for index, row in df.iterrows(): #Iterates across the dataframe\n",
        "        file_path = os.path.join(folder_path, row['path']) #Grabs one audio file\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        spectrogram = convert_to_mel_spectrogram(file_path, n_mels, target_length) # Calls above function on the single audio file\n",
        "        if spectrogram is not None: #If there are already spectrograms in the list, make this spectrogram the same size as that\n",
        "            if max_frames is None:\n",
        "                max_frames = spectrogram.shape[1]\n",
        "            elif spectrogram.shape[1] > max_frames: #If there are no spectrograms in the list, we use the first as model for the rest.\n",
        "                max_frames = spectrogram.shape[1]\n",
        "\n",
        "            spectrograms.append(spectrogram) #Adds spectrogram to list of spectrograms\n",
        "\n",
        "    #Again, insures all spectrograms have the same width. Generated with GPT. This pads spectrograms if necessary. (Padding should not be added in rows(0,0) but in columns until max_frames size)\n",
        "    spectrograms = [np.pad(spec, ((0, 0), (0, max_frames - spec.shape[1])), 'constant', constant_values=-80) if spec.shape[1] < max_frames else spec for spec in spectrograms]\n",
        "\n",
        "    return np.array(spectrograms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpD4BmsqwUo"
      },
      "source": [
        "#German File Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkERAd3bsGqq"
      },
      "source": [
        "We will now process the German Files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CmxzGrVsWwF"
      },
      "source": [
        "First, we call the above functions on our German audio files. We define the path to access the drive folder and only want the fields listed in the balanced dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M28cO3QXqw-z",
        "outputId": "b2580070-2bbb-4118-8c6b-6ac080d1090c"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Defines path and processes data\n",
        "folder = 'de'\n",
        "folder_path = os.path.join(path_to_datasets, folder, 'clips/clips')\n",
        "\n",
        "# Runs audio files through above functions.\n",
        "train_de_spectrograms = process_audio_files_from_df(balanced_de_train, folder_path)\n",
        "test_de_spectrograms = process_audio_files_from_df(balanced_de_test, folder_path)\n",
        "\n",
        "print(\"Finished processing all files.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_tgYHEMb83C"
      },
      "source": [
        "We then convert the data from arrays to torch tensors. We also print the shape to prepare for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGCzRuPROoq4",
        "outputId": "c78d65a2-73bf-41ec-b249-9fab60af14e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "deDataTrain1 = torch.tensor(train_de_spectrograms, dtype=torch.float32) #Converts training data to tensor\n",
        "print(np.shape(deDataTrain1)) #Prints shape of data training set\n",
        "\n",
        "deDataTest1 = torch.tensor(test_de_spectrograms,dtype=torch.float32) #Converts testing data to tensor\n",
        "print(np.shape(german_labels_train)) # Prints shape of labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfu4Xu8doDuk"
      },
      "source": [
        "Below prepares the dataset to be input to the model. First,we need to flatten the data from [200,128,188] to size [200, x], where x = 128*188. Then we yet again normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhSyweinEZ17"
      },
      "outputs": [],
      "source": [
        "deDataTrain = torch.reshape(deDataTrain1, (deDataTrain1.size(0), -1)) # Flatten to [500, 128*188] #Used GPT to help reshape\n",
        "deDataTest = torch.reshape(deDataTest1, (deDataTest1.size(0), -1))  # Flatten to [400, 128*188](aka [400,24064])\n",
        "\n",
        "#Normalizes dataset as better input into model #Used GPT to normalize\n",
        "deDataTrain = (deDataTrain - torch.mean(deDataTrain)) / torch.std(deDataTrain)\n",
        "deDataTest = (deDataTest - torch.mean(deDataTest)) / torch.std(deDataTest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o1Shl18ev93"
      },
      "source": [
        "## The Model -- German"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83yVUAu3_gEc"
      },
      "source": [
        "This is the model we will use. This is largely adapted from our in class model. We use the BCEWithLogitsLoss function, which has the sigmoid function built in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXJMGXNiV13E",
        "outputId": "d14bb5e7-81b8-424e-ec43-4837d8051adb"
      },
      "outputs": [],
      "source": [
        "#Creates Model\n",
        "torch.manual_seed(12) #Sets seed to 12\n",
        "\n",
        "modelAudio = nn.Sequential(  # nn.Sequential() stacks all of our neural network layers together.\n",
        "                        #   inside nn.Sequential(), specify layers of the network: alternating linear layers,\n",
        "                        #   which do matrix multiplication, and nonlinear layers, which implement the activation\n",
        "                       #   function (ReLU in this case)\n",
        "    nn.Linear(24064, 5092),     # linear layer that takes 60032 inputs and projects to 5092 nodes in the next layer(60032 measurements for each sample)\n",
        "    nn.ReLU(),            # nonlinear ReLU for all nodes\n",
        "    nn.Linear(5092, 1024),     # linear layer that takes 5092 inputs and projects to 1024 nodes in the next layer\n",
        "    nn.ReLU(),            # nonlinear ReLU for all nodes\n",
        "    nn.Linear(1024, 512),     # linear layer that takes 1024 inputs and projects to 512 nodes in the next layer\n",
        "    nn.ReLU(),               # nonlinear ReLU for all nodes\n",
        "    nn.Linear(512, 128),     # linear layer that takes 512 inputs and projects to 128 nodes in the next layer\n",
        "    nn.ReLU(),              # nonlinear ReLU for all nodes\n",
        "    nn.Linear(128, 8),      # linear layer that takes 128 inputs and projects to 8 node in the next layer\n",
        "    nn.ReLU(),               # nonlinear ReLU for all nodes\n",
        "    nn.Linear(8, 1),    # linear layer that takes 8 inputs and projects to 1 final outputs\n",
        "\n",
        "\n",
        ")  # close nn.Sequential()\n",
        "\n",
        "print(modelAudio) #Prints the model\n",
        "loss_fn   = torch.nn.BCEWithLogitsLoss() #Defines loss function. This is good for binary classification\n",
        "optimizer = optim.Adam(modelAudio.parameters(), lr=.00012)  # Adam (ADAptive Moment estimation) optimization algorithm -- a variant of gradient descent\n",
        "                                                      # set learning rate ('lr') inside function\n",
        "\n",
        "nIts = 10\n",
        "\n",
        "#Creates empty numpy arrays in the shape of the number of iterations to save the iteration number, loss of training set, and loss of testing set.\n",
        "iteration = np.zeros(nIts)\n",
        "lossTrain = np.zeros(nIts)\n",
        "lossTest = np.zeros(nIts)\n",
        "\n",
        "for it in range(nIts):      # for each iteration of optimization\n",
        "    y_pred = modelAudio(deDataTrain)          # predict model outputs based on current weights ('forward step')\n",
        "    loss = loss_fn(y_pred, german_labels_train)  # calculate the loss, based on model current predictions (y_pred) and desired outputs\n",
        "    lossTrain[it] = loss.detach().numpy() #Adds training loss to loss array, ensuring it is not a tensor\n",
        "    iteration[it] = it         #Adds iteration to numpy array\n",
        "    optimizer.zero_grad()      # reset all gradient values to 0\n",
        "    loss.backward()            # calculate new gradient values for each weight based on current loss ('backward step')\n",
        "    optimizer.step()           # update weights based on the optimization algorithm we chose above\n",
        "\n",
        "    with torch.no_grad(): #Doesn't calculate grad for testing\n",
        "        y_pred_test = modelAudio(deDataTest) #Runs model on testing\n",
        "        loss_test = loss_fn(y_pred_test, german_labels_test) #Runs loss on test set\n",
        "        lossTest[it] = loss_test.item() #Saves result of loss\n",
        "\n",
        "\n",
        "    # Optionally, print the training and testing loss for monitoring from GPT recommendation on formatting\n",
        "    if it % 2 == 0:\n",
        "        print(f'Iteration {it}: Training Loss = {lossTrain[it]}, Testing Loss = {lossTest[it]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_aLSSKx83RQO",
        "outputId": "8def13e8-7193-4ea1-91f5-9a128c8eb575"
      },
      "outputs": [],
      "source": [
        "#Plots loss vs iteration for German\n",
        "plt.figure() #Creates figure\n",
        "plt.plot(iteration, lossTrain, label = \"train\") #Plots training data\n",
        "plt.plot(iteration, lossTest, label = 'test') #Plots testing data\n",
        "plt.title(\"Loss vs Iteration German\") #Creates title\n",
        "plt.legend() #Creates legend\n",
        "plt.xlabel(\"Iteration\") #X axis label\n",
        "plt.ylabel(\"Loss\"); # Y axis label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnkFJcM9_0e2"
      },
      "source": [
        "Below is the function to calculate accuracy, which applies the sigmoid function to convert the logits output by the model to probabilities between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54fJOo-jLO0e",
        "outputId": "c49ddd25-784f-418e-8d0d-2559b6c6a9ff"
      },
      "outputs": [],
      "source": [
        "#Generated through a combination of GPT, classwork, and https://stackoverflow.com/questions/75979632/pytorchs-nn-bcewithlogitsloss-behaves-totaly-differently-than-nn-bceloss\n",
        "def compute_accuracy(predictions, labels): #Computes the accuracy\n",
        "\n",
        "    # Threshold predictions at 0.5 to determine class\n",
        "    predicted_classes = (predictions > .5).float()\n",
        "    correct = (predicted_classes == labels).float()  # Compares each element of predicted to the labels. Then finds the mean(overall accuracy)\n",
        "    accuracy = correct.mean()  # Mean of correct predictions using\n",
        "    return accuracy\n",
        "\n",
        "#Runs model and calclates accuracy for German Data\n",
        "y_pred_train = modelAudio(deDataTrain) #Runs model on training data\n",
        "train_accuracy = compute_accuracy(y_pred_train, german_labels_train) #Calculates accuracy for German files\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\") #Prints accuracy for training set to 4 decimal places\n",
        "\n",
        "with torch.no_grad():  # Ensure no gradients are computed during inference\n",
        "  y_pred_test = modelAudio(deDataTest) #Same as above with test data\n",
        "  test_accuracy = compute_accuracy(y_pred_test, german_labels_test)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJmCdPJ4omLR"
      },
      "source": [
        "#English File Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjSWflIkAxj2"
      },
      "source": [
        "The below section does the same as the German section, only with the English data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaXyFBhZooxI",
        "outputId": "007393c4-0f1b-41ad-9f2c-cf38aec9cabd"
      },
      "outputs": [],
      "source": [
        "# Defines path for dataset. Here we are in the english folder\n",
        "folder = 'en'\n",
        "folder_path = os.path.join(path_to_datasets, folder, 'clips/clips')\n",
        "\n",
        "# Converts df to mel spectrogram and then to numpy array\n",
        "train_en_spectrograms = process_audio_files_from_df(balanced_en_train, folder_path)\n",
        "test_en_spectrograms = process_audio_files_from_df(balanced_en_test, folder_path)\n",
        "\n",
        "print(\"Finished processing all files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQaky88DokSs",
        "outputId": "c67efcd1-31b6-411a-f54e-8a240bbb6b3f"
      },
      "outputs": [],
      "source": [
        "enDataTrain1 = torch.tensor(train_en_spectrograms, dtype=torch.float32) #Converts training data to tensor\n",
        "print(np.shape(enDataTrain1)) #Prints shape of data training set\n",
        "enDataTest1 = torch.tensor(test_en_spectrograms,dtype=torch.float32) #Converts testing data to tensor\n",
        "\n",
        "print(np.shape(english_labels_train)) # Prints shape of labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2tzzWmD5WtC"
      },
      "source": [
        "Below prepares the dataset to be input to the model. First,we need to flatten the data from [200,128,188] to size [200, x], where x = 128*188. Then we yet again normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pARLd20-pEO9"
      },
      "outputs": [],
      "source": [
        "enDataTrain = torch.reshape(enDataTrain1, (enDataTrain1.size(0), -1)) # Flatten to [400, 128*188] #Used GPT to help reshape\n",
        "enDataTest = torch.reshape(enDataTest1, (enDataTest1.size(0), -1))  # Flatten to [400, 128*188]\n",
        "\n",
        "#Normalizes english data, simiar to German data\n",
        "enDataTrain = (enDataTrain - torch.mean(enDataTrain)) / torch.std(enDataTrain)\n",
        "enDataTest = (enDataTest - torch.mean(enDataTest)) / torch.std(enDataTest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwuHZJ8JBD8b"
      },
      "source": [
        "## The Model -- English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moJXkMKJ8ti4"
      },
      "source": [
        "This is the model we will use. This is largely adapted from our in class model. We use the BCEWithLogitsLoss function, which has the sigmoid function built in. This is the same as the German model, only with the English data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wLD6_OEpTtU",
        "outputId": "3c5b0097-819a-4c79-88b5-026a1dd3b832"
      },
      "outputs": [],
      "source": [
        "#Creates Model\n",
        "torch.manual_seed(12) #Sets seed to 12\n",
        "\n",
        "modelAudio = nn.Sequential(  # nn.Sequential() stacks all of our neural network layers together.\n",
        "                        #   inside nn.Sequential(), specify layers of the network: alternating linear layers,\n",
        "                        #   which do matrix multiplication, and nonlinear layers, which implement the activation\n",
        "                       #   function (ReLU in this case)\n",
        "    nn.Linear(24064, 5092),     # linear layer that takes 60032 inputs and projects to 5092 nodes in the next layer(60032 measurements for each sample)\n",
        "    nn.ReLU(),            # nonlinear ReLU for all nodes\n",
        "    nn.Linear(5092, 1024),     # linear layer that takes 5092 inputs and projects to 1024 nodes in the next layer\n",
        "    nn.ReLU(),            # nonlinear ReLU for all nodes\n",
        "    nn.Linear(1024, 512),     # linear layer that takes 1024 inputs and projects to 512 nodes in the next layer\n",
        "    nn.ReLU(),               # nonlinear ReLU for all nodes\n",
        "    nn.Linear(512, 128),     # linear layer that takes 512 inputs and projects to 128 nodes in the next layer\n",
        "    nn.ReLU(),              # nonlinear ReLU for all nodes\n",
        "    nn.Linear(128, 8),      # linear layer that takes 128 inputs and projects to 8 node in the next layer\n",
        "    nn.ReLU(),               # nonlinear ReLU for all nodes\n",
        "    nn.Linear(8, 1),    # linear layer that takes 8 inputs and projects to 1 final outputs\n",
        "\n",
        ")  # close nn.Sequential()\n",
        "\n",
        "print(modelAudio) #Prints the model\n",
        "loss_fn   = torch.nn.BCEWithLogitsLoss() #Defines loss function. This is good for binary classification\n",
        "optimizer = optim.Adam(modelAudio.parameters(), lr=.00012)  # Adam (ADAptive Moment estimation) optimization algorithm -- a variant of gradient descent\n",
        "                                                      # set learning rate ('lr') inside function\n",
        "\n",
        "nIts = 10\n",
        "\n",
        "#Creates empty numpy arrays in the shape of the number of iterations to save the iteration number, loss of training set, and loss of testing set.\n",
        "iteration = np.zeros(nIts)\n",
        "lossTrain = np.zeros(nIts)\n",
        "lossTest = np.zeros(nIts)\n",
        "\n",
        "for it in range(nIts):      # for each iteration of optimization\n",
        "    y_pred = modelAudio(enDataTrain)          # predict model outputs based on current weights ('forward step')\n",
        "    loss = loss_fn(y_pred, english_labels_train)  # calculate the loss, based on model current predictions (y_pred) and desired outputs\n",
        "    lossTrain[it] = loss.detach().numpy() #Adds training loss to loss array, ensuring it is not a tensor\n",
        "    iteration[it] = it         #Adds iteration to numpy array\n",
        "    optimizer.zero_grad()      # reset all gradient values to 0\n",
        "    loss.backward()            # calculate new gradient values for each weight based on current loss ('backward step')\n",
        "    optimizer.step()           # update weights based on the optimization algorithm we chose above\n",
        "\n",
        "\n",
        "    #modelAudio.eval()  #  Used GPT: Set the model to evaluation mode\n",
        "    with torch.no_grad(): #Doesn't calculate grad for testing\n",
        "        y_pred_test = modelAudio(enDataTest) #Runs model on testing\n",
        "        loss_test = loss_fn(y_pred_test, english_labels_test) #Runs loss on test set\n",
        "        lossTest[it] = loss_test.item() #Saves result of loss\n",
        "\n",
        "    # Prints loss every 2 iterations\n",
        "    if it % 2 == 0:\n",
        "        print(f'Iteration {it}: Training Loss = {lossTrain[it]}, Testing Loss = {lossTest[it]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLRTxqZJ7-fI"
      },
      "source": [
        "Below is the Loss vs Iteration plot for English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56bax1od3_Mh",
        "outputId": "7d75779d-39fe-40dc-96f0-d259a7dae815"
      },
      "outputs": [],
      "source": [
        "#Plots loss vs iteration for English\n",
        "plt.figure() #Creates figure\n",
        "plt.plot(iteration, lossTrain, label = \"train\") #Plots training data\n",
        "plt.plot(iteration, lossTest, label = 'test') #Plots testing data\n",
        "plt.title(\"Loss vs Iteration English\") #Creates title\n",
        "plt.legend() #Creates legend\n",
        "plt.xlabel(\"Iteration\") #X axis label\n",
        "plt.ylabel(\"Loss\"); # Y axis label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkG04xWi8Ce3"
      },
      "source": [
        "Finally, here is the accuracy calculation, which calls the accuracy calculation function in the German section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCf4PsQnqCrT",
        "outputId": "efe095ca-c883-4184-e47d-8ece3d8f64e7"
      },
      "outputs": [],
      "source": [
        "y_pred_train = modelAudio(enDataTrain) #Runs model on training data\n",
        "train_accuracy = compute_accuracy(y_pred_train, english_labels_train) #Calculates accuracy\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\") #Prints the accuracy in string form(instead of torch tensor)\n",
        "\n",
        "with torch.no_grad():  # No gradients calculated during testing\n",
        "  y_pred_test = modelAudio(enDataTest) #Runs model on test data\n",
        "  test_accuracy = compute_accuracy(y_pred_test, english_labels_test) #Calls accuracy function\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\") #Prints the accuracy in string form(instead of torch tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnd6CBeuiSOY"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDJUJOcNjjfm"
      },
      "source": [
        "Better than chance, but not by a lot. Best ran case: .00012. German is consistantly more accurate than English(Exception = .001, .008)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQDVKn0ViUA8"
      },
      "source": [
        "\n",
        "Learning Rate = .0001\n",
        "*  German Data: Train Accuracy: 0.7125\n",
        "Test Accuracy: 0.65\n",
        "*   English Data: Train Accuracy: 0.6475\n",
        "Test Accuracy: 0.5625\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzHP5VqWj3JY"
      },
      "source": [
        "\n",
        "Learning Rate = .001\n",
        "*  German Data: Train Accuracy: 0.615\n",
        "Test Accuracy: 0.55\n",
        "*   English Data: Train Accuracy: 0.59\n",
        "Test Accuracy: 0.5625\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te-U61Y2tihW"
      },
      "source": [
        "\n",
        "Learning Rate = .008\n",
        "*  German Data: Train Accuracy: 0.5\n",
        "Test Accuracy: 0.5\n",
        "*   English Data: Train Accuracy: 0.5\n",
        "Test Accuracy: 0.5\n",
        "(Seemed incorrect but tried with other learning rates and it produced other values, thus accuracy is just .5 with .008)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10sORLJEzyGZ"
      },
      "source": [
        "Learning Rate = .00012\n",
        "\n",
        "\n",
        "*   German Data: Train Accuracy: 0.740 Test Accuracy: 0.6725\n",
        "*   English Data: Train Accuracy: 0.64 Test Accuracy: 0.575\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAILHhvz7nOB"
      },
      "source": [
        "#Reflecting Questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjWxXNUtKPeO"
      },
      "source": [
        "What does this project demonstrate?\n",
        "\n",
        "\n",
        "*   Difference in language accuracy\n",
        "*   We can use simple NN model(instead of CNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiPGq3FynumR"
      },
      "source": [
        "What did I change to fix the big error?  normalized yet again, passed another\n",
        "\n",
        "*   Normalized again\n",
        "*   Passed another parameter into spectrogram\n",
        "*   Fixed a padding error(was 5 seconds in 1 function where it should have been 2)\n",
        "*   Reran model from top to finish. If I did not restart session and ran model again, got .5 accuracy. Restarting session solves this issue. Also learning rate of .008 gives .5 accuracy. Running on another lr confirmed this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJbiCDSER_v"
      },
      "source": [
        "How did I convince myself my code was working properly?\n",
        "\n",
        "1.   Accuracy was exactly .5 -- Seemed off. When this changed and my loss function decreased- good sign!\n",
        "2.   Looked more closely at previous implimentations and confirmed my functions closely matched theirs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wdY3IqEn0n"
      },
      "source": [
        "What was the hardest part of this project?\n",
        "\n",
        "1.   Solving the consistant accuracy error problem\n",
        "2.   Preprocessing the data and confirming padding, dimensions, timing was correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wA9BfgqDWF5"
      },
      "source": [
        "What would I do differently?\n",
        "\n",
        "1.   Work with 1 language first\n",
        "2.   Closely followed what is input into Mel Spectrogram\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn-tZIKCFPla"
      },
      "source": [
        "What steps would I take with more time?\n",
        "\n",
        "\n",
        "1.   Try a better model(More iterations, data, learning rate, CNN) or loss function.\n",
        "2.   Impliment this on other languages, including other genders(other) with multiclass, longer phrases, or segments with multiple speakers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUw24bTo7wqC"
      },
      "source": [
        "What libraries did I use and what are the important features?\n",
        "\n",
        "Important libraries used:\n",
        "\n",
        "\n",
        "*  Librosa - Mel Spectrograms. Important functions: # Passing through arguments to the Mel filters S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000) , S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "*   Pandas: Great for processing dataframes\n",
        "*   Pydub: Audio processing with .frame_rate, .get_array_with_samples\n",
        "\n",
        "\n",
        "\n",
        "Loss function:\n",
        "BCE Loss: Applies sigmoid to model output(turns output(logits) into probabilities) and then measures distance between true labels and probability. This is the mathematical process, but in reality, are done at the same time."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
